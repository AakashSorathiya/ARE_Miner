{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.vocab import GloVe\n",
    "import torchcrf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2062 entries, 0 to 2061\n",
      "Data columns (total 11 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Unnamed: 0               2062 non-null   int64 \n",
      " 1   App id                   2062 non-null   object\n",
      " 2   Review id                2062 non-null   object\n",
      " 3   Sentence id              2062 non-null   int64 \n",
      " 4   Sentence content         2062 non-null   object\n",
      " 5   Feature (Positive)       291 non-null    object\n",
      " 6   Feature (Neutral)        638 non-null    object\n",
      " 7   Feature (Negative)       110 non-null    object\n",
      " 8   Feature (All Annotated)  971 non-null    object\n",
      " 9   clean_content            2062 non-null   object\n",
      " 10  tags                     2062 non-null   object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 177.3+ KB\n"
     ]
    }
   ],
   "source": [
    "truth_dataset = pd.read_csv('../datafiles/true_tags.csv')\n",
    "truth_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_dataset['clean_content'] = truth_dataset['clean_content'].apply(ast.literal_eval)\n",
    "truth_dataset['tags'] = truth_dataset['tags'].apply(ast.literal_eval)\n",
    "truth_dataset = truth_dataset[truth_dataset['clean_content'].apply(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "App id\n",
       "B004LOMB2Q                    367\n",
       "B005ZXWMUS                    341\n",
       "B0094BB4TW                    327\n",
       "B004SIIBGU                    294\n",
       "com.spotify.music             226\n",
       "com.twitter.android           183\n",
       "com.whatsapp                  169\n",
       "com.zentertain.photoeditor    154\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_dataset['App id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(truth_dataset[truth_dataset['App id']=='B004LOMB2Q']['clean_content'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2061\n",
      "2061\n"
     ]
    }
   ],
   "source": [
    "all_sentences = truth_dataset['clean_content'].to_list()\n",
    "all_tags = truth_dataset['tags'].to_list()\n",
    "\n",
    "print(len(all_sentences))\n",
    "print(len(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {word: i+1 for i, word in enumerate(set([w for s in all_sentences for w in s]))}\n",
    "word_to_ix['<PAD>']=0\n",
    "word_to_ix['<UNK>']=len(word_to_ix)\n",
    "tag_to_ix = {'<PAD>': 0, 'B': 1, 'I': 2, 'O': 3}\n",
    "ix_to_tag = {ix: tag for tag, ix in tag_to_ix.items()}\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentences, tags, word_to_ix, tag_to_ix, pad_idx=0):\n",
    "    max_len = max(len(s) for s in sentences)\n",
    "    \n",
    "    sentences_idx = [[word_to_ix[word] for word in sent] + [pad_idx] * (max_len - len(sent)) for sent in sentences]\n",
    "    tags_idx = [[tag_to_ix[tag] for tag in tag_seq] + [pad_idx] * (max_len - len(tag_seq)) for tag_seq in tags]\n",
    "    \n",
    "    sentences_tensor = torch.tensor(sentences_idx, dtype=torch.long)\n",
    "    tags_tensor = torch.tensor(tags_idx, dtype=torch.long)\n",
    "    \n",
    "    return TensorDataset(sentences_tensor, tags_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences, test_sentences, dev_tags, test_tags = train_test_split(all_sentences, all_tags, test_size=0.2, random_state=42)\n",
    "train_sentences, val_sentences, train_tags, val_tags = train_test_split(dev_sentences, dev_tags, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = prepare_data(train_sentences, train_tags, word_to_ix, tag_to_ix)\n",
    "val_data = prepare_data(val_sentences, val_tags, word_to_ix, tag_to_ix)\n",
    "test_data = prepare_data(test_sentences, test_tags, word_to_ix, tag_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B004LOMB2Q\n",
      "1694 367\n",
      "B005ZXWMUS\n",
      "1720 341\n",
      "B0094BB4TW\n",
      "1734 327\n",
      "B004SIIBGU\n",
      "1767 294\n",
      "com.spotify.music\n",
      "1835 226\n",
      "com.twitter.android\n",
      "1878 183\n",
      "com.whatsapp\n",
      "1892 169\n",
      "com.zentertain.photoeditor\n",
      "1907 154\n"
     ]
    }
   ],
   "source": [
    "cross_domain_data = []\n",
    "for app in truth_dataset['App id'].value_counts().keys():\n",
    "    print(app)\n",
    "    dev_sentences = truth_dataset[truth_dataset['App id']!=app]['clean_content'].to_list()\n",
    "    dev_tags = truth_dataset[truth_dataset['App id']!=app]['tags'].to_list()\n",
    "    test_sentences = truth_dataset[truth_dataset['App id']==app]['clean_content'].to_list()\n",
    "    test_tags = truth_dataset[truth_dataset['App id']==app]['tags'].to_list()\n",
    "\n",
    "    print(len(dev_sentences), len(test_sentences))\n",
    "\n",
    "    train_sentences, val_sentences, train_tags, val_tags = train_test_split(dev_sentences, dev_tags, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_data = prepare_data(train_sentences, train_tags, word_to_ix, tag_to_ix)\n",
    "    val_data = prepare_data(val_sentences, val_tags, word_to_ix, tag_to_ix)\n",
    "    test_data = prepare_data(test_sentences, test_tags, word_to_ix, tag_to_ix)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    cross_domain_data.append({'app': app, 'train_loader': train_loader, 'val_loader': val_loader, 'test_loader': test_loader, 'test_sentences': test_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, glove_embeddings):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_embeddings, freeze=False)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs)\n",
    "        out, hidden = self.encoder(embeds)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, tagset_size) -> None:\n",
    "        super().__init__()\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "    def forward(self, decoder_inputs):\n",
    "        out, hidden = self.decoder(decoder_inputs)\n",
    "        emissions = self.fc(out)\n",
    "        return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        attention_weights = torch.tanh(self.attention(input))\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        out = input * attention_weights\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, tagset_size, encoder, decoder, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = attention\n",
    "        self.crf = torchcrf.CRF(tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        out, _ = self.encoder(sentence)\n",
    "        out = self.attention(out)\n",
    "        emissions = self.decoder(out)\n",
    "        \n",
    "        return emissions\n",
    "    \n",
    "    def loss(self, emissions, tags, mask=None):\n",
    "        return -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "    \n",
    "    def decode(self, emissions, mask=None):\n",
    "        return self.crf.decode(emissions, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 512\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "TAGSET_SIZE = len(tag_to_ix)\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
    "\n",
    "glove_embeddings = torch.zeros(VOCAB_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2818, 300])\n"
     ]
    }
   ],
   "source": [
    "for word, idx in word_to_ix.items():\n",
    "    if word in glove.stoi:\n",
    "        glove_embeddings[idx] = glove[word]\n",
    "    else:\n",
    "        glove_embeddings[idx] = torch.randn(EMBEDDING_DIM)\n",
    "\n",
    "print(glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BiLSTMEncoder(EMBEDDING_DIM, HIDDEN_DIM, glove_embeddings)\n",
    "decoder = LSTMDecoder(HIDDEN_DIM, TAGSET_SIZE)\n",
    "attention = SelfAttention(HIDDEN_DIM)\n",
    "model = Seq2SeqModel(TAGSET_SIZE, encoder, decoder, attention)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sentences_batch, tags_batch in train_loader:\n",
    "        mask = (sentences_batch != 0)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emissions = model(sentences_batch)\n",
    "        # print(emissions.shape, tags_batch.shape)\n",
    "        loss = model.loss(emissions, tags_batch, mask)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Train Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sentences_batch, tags_batch in val_loader:\n",
    "            mask = (sentences_batch != 0)\n",
    "            emissions = model(sentences_batch)\n",
    "            loss = model.loss(emissions, tags_batch, mask)\n",
    "            total_loss += loss.item()\n",
    "    print(f\"Validation Loss: {total_loss / len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_loop(model, train_loader, val_loader, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch}')\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase=='train':\n",
    "                train_model(model, train_loader, optimizer)\n",
    "            else:\n",
    "                evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    masks = []\n",
    "    with torch.no_grad():\n",
    "        for sentences_batch, _ in test_loader:\n",
    "            mask = (sentences_batch != 0)\n",
    "            emissions = model(sentences_batch)\n",
    "            predictions = model.decode(emissions, mask=mask)\n",
    "            pred_tags = [[ix_to_tag[t] for t in seq] for seq in predictions]\n",
    "            \n",
    "            all_predictions.extend(pred_tags)\n",
    "            masks.extend(mask)\n",
    "    \n",
    "    return all_predictions, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_1(predictions, true_tags, true_tokens):\n",
    "    \n",
    "    # print(len(predictions), len(true_tags), len(test_sentences))\n",
    "    levels = 3\n",
    "    \n",
    "    def extract_entities(seq, sentence):\n",
    "\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for i, tag in enumerate(seq):\n",
    "            if tag == 'B':\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                current_entity = [sentence[i]]\n",
    "            elif tag == 'I':\n",
    "                if current_entity is None:\n",
    "                    current_entity = [sentence[i]]\n",
    "                else:\n",
    "                    current_entity.append(sentence[i])\n",
    "            elif tag == 'O':\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "        \n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def is_match(f1, f2, n):\n",
    "        \"\"\"\n",
    "        Check if two features match at level n.\n",
    "        Conditions:\n",
    "        1. One feature is equal to or is a subset of the other\n",
    "        2. Absolute length difference is at most n\n",
    "        \"\"\"\n",
    "        f1=set(f1)\n",
    "        f2=set(f2)\n",
    "        \n",
    "        is_subset = f1.issubset(f2) or f2.issubset(f1)\n",
    "        length_diff = abs(len(f1) - len(f2))\n",
    "        \n",
    "        return is_subset and length_diff <= n\n",
    "\n",
    "\n",
    "    all_true_entites = []\n",
    "    all_pred_entites = []\n",
    "    \n",
    "    for pred_seq, true_seq, token_seq in zip(predictions, true_tags, true_tokens):\n",
    "        true_entities = extract_entities(true_seq, token_seq)\n",
    "        pred_entities = extract_entities(pred_seq, token_seq)\n",
    "        # print(pred_entities)\n",
    "        # print(true_entities)\n",
    "\n",
    "        all_true_entites.append(true_entities)\n",
    "        all_pred_entites.append(pred_entities)\n",
    "\n",
    "    total_true = len(all_true_entites)\n",
    "    total_pred = len(all_pred_entites)\n",
    "    metrics = {}\n",
    "    # print(total_pred, total_true, all_levels_TPs)\n",
    "\n",
    "    for level in range(levels):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "\n",
    "        for true_entities, pred_entities in zip(all_true_entites, all_pred_entites):   \n",
    "            matched_true = set()\n",
    "            for pred_entity in pred_entities:\n",
    "                found_match = False\n",
    "                \n",
    "                for i, true_entity in enumerate(true_entities):\n",
    "                    if i not in matched_true and is_match(pred_entity, true_entity, level):\n",
    "                        tp += 1\n",
    "                        matched_true.add(i)\n",
    "                        found_match = True\n",
    "                        break\n",
    "                \n",
    "                if not found_match:\n",
    "                    fp += 1\n",
    "\n",
    "            fn += len(true_entities) - len(matched_true)\n",
    "            # print(fn)\n",
    "    \n",
    "        print(level, tp, fp, fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        level_name = ['exact', 'n-1', 'n-2'][level]\n",
    "        metrics.update({\n",
    "            f'{level_name}_precision': precision,\n",
    "            f'{level_name}_recall': recall,\n",
    "            f'{level_name}_f1': f1\n",
    "        })\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model_across_domains(epochs):\n",
    "    results=[]\n",
    "    \n",
    "    for data in cross_domain_data:\n",
    "        # print(f'For app: {data['app']}')\n",
    "        # model = Seq2SeqModel(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGSET_SIZE, glove_embeddings)\n",
    "        # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        train_eval_loop(model, data['train_loader'], data['val_loader'], optimizer, epochs)\n",
    "        test_predictions, masks = test_model(model, test_loader)\n",
    "        f1_scores = calculate_metrics_1(test_predictions, test_tags, test_sentences)\n",
    "        results.append({data['app']: f1_scores})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train Loss: 7.988963670508806\n",
      "Validation Loss: 5.573008710687811\n",
      "Epoch: 1\n",
      "Train Loss: 4.58016205942908\n",
      "Validation Loss: 4.424422762610695\n",
      "Epoch: 2\n",
      "Train Loss: 3.3505957237509794\n",
      "Validation Loss: 3.6663234017112036\n",
      "Epoch: 3\n",
      "Train Loss: 2.635633008424626\n",
      "Validation Loss: 3.391868764703924\n",
      "Epoch: 4\n",
      "Train Loss: 2.335899060548738\n",
      "Validation Loss: 3.419276064092463\n",
      "Epoch: 5\n",
      "Train Loss: 1.8843269223390624\n",
      "Validation Loss: 3.524521372535012\n",
      "Epoch: 6\n",
      "Train Loss: 1.6200790114180987\n",
      "Validation Loss: 3.5522937124425713\n",
      "Epoch: 7\n",
      "Train Loss: 1.2247156819631888\n",
      "Validation Loss: 3.8435129902579566\n",
      "Epoch: 8\n",
      "Train Loss: 0.9841157489044722\n",
      "Validation Loss: 3.962268049066717\n",
      "Epoch: 9\n",
      "Train Loss: 0.8329973096071288\n",
      "Validation Loss: 4.684133031151512\n",
      "0 73 15 18\n",
      "1 80 8 11\n",
      "2 82 6 9\n",
      "Epoch: 0\n",
      "Train Loss: 1.962629240612651\n",
      "Validation Loss: 2.5443329377607866\n",
      "Epoch: 1\n",
      "Train Loss: 1.3920310680256334\n",
      "Validation Loss: 2.8637360117652197\n",
      "Epoch: 2\n",
      "Train Loss: 0.9722323313701985\n",
      "Validation Loss: 4.249665390361439\n",
      "Epoch: 3\n",
      "Train Loss: 0.8168401537939559\n",
      "Validation Loss: 3.7860825061798096\n",
      "Epoch: 4\n",
      "Train Loss: 0.7470577518607295\n",
      "Validation Loss: 4.276784810152921\n",
      "Epoch: 5\n",
      "Train Loss: 0.6263002545334572\n",
      "Validation Loss: 4.2476547306234185\n",
      "Epoch: 6\n",
      "Train Loss: 0.5618890406087388\n",
      "Validation Loss: 4.674171902916648\n",
      "Epoch: 7\n",
      "Train Loss: 0.5165107936360115\n",
      "Validation Loss: 4.876138470389626\n",
      "Epoch: 8\n",
      "Train Loss: 0.4711874667988267\n",
      "Validation Loss: 4.80186052755876\n",
      "Epoch: 9\n",
      "Train Loss: 0.4423015283983807\n",
      "Validation Loss: 6.472054329785434\n",
      "0 62 18 29\n",
      "1 66 14 25\n",
      "2 74 6 17\n",
      "Epoch: 0\n",
      "Train Loss: 1.086433439769528\n",
      "Validation Loss: 2.8800933252681387\n",
      "Epoch: 1\n",
      "Train Loss: 0.665584425357255\n",
      "Validation Loss: 3.1782757314768704\n",
      "Epoch: 2\n",
      "Train Loss: 0.5363946970213543\n",
      "Validation Loss: 3.533275463364341\n",
      "Epoch: 3\n",
      "Train Loss: 0.4493538038805127\n",
      "Validation Loss: 3.5636002854867415\n",
      "Epoch: 4\n",
      "Train Loss: 0.39300384220074525\n",
      "Validation Loss: 3.63069611787796\n",
      "Epoch: 5\n",
      "Train Loss: 0.33749097247015347\n",
      "Validation Loss: 3.921727928248319\n",
      "Epoch: 6\n",
      "Train Loss: 0.29010989144444466\n",
      "Validation Loss: 4.57848025452007\n",
      "Epoch: 7\n",
      "Train Loss: 0.3488105060532689\n",
      "Validation Loss: 4.977436455813321\n",
      "Epoch: 8\n",
      "Train Loss: 0.36896727606654167\n",
      "Validation Loss: 4.635441769253124\n",
      "Epoch: 9\n",
      "Train Loss: 0.20018929514017972\n",
      "Validation Loss: 4.4546896110881455\n",
      "0 79 7 12\n",
      "1 83 3 8\n",
      "2 84 2 7\n",
      "Epoch: 0\n",
      "Train Loss: 0.6624581399891112\n",
      "Validation Loss: 2.162811746199926\n",
      "Epoch: 1\n",
      "Train Loss: 0.4482748144202762\n",
      "Validation Loss: 2.56108749906222\n",
      "Epoch: 2\n",
      "Train Loss: 0.2900975015428331\n",
      "Validation Loss: 2.887575844923655\n",
      "Epoch: 3\n",
      "Train Loss: 0.19832679171943002\n",
      "Validation Loss: 3.089319884777069\n",
      "Epoch: 4\n",
      "Train Loss: 0.17377355402956407\n",
      "Validation Loss: 3.042494535446167\n",
      "Epoch: 5\n",
      "Train Loss: 0.19851784222604085\n",
      "Validation Loss: 2.8167006770769754\n",
      "Epoch: 6\n",
      "Train Loss: 0.1463988588915931\n",
      "Validation Loss: 3.33493572473526\n",
      "Epoch: 7\n",
      "Train Loss: 0.13281794918908013\n",
      "Validation Loss: 3.497522791226705\n",
      "Epoch: 8\n",
      "Train Loss: 0.1524848339872228\n",
      "Validation Loss: 3.333733379840851\n",
      "Epoch: 9\n",
      "Train Loss: 0.10234918428791893\n",
      "Validation Loss: 4.773078948259354\n",
      "0 71 15 20\n",
      "1 75 11 16\n",
      "2 80 6 11\n",
      "Epoch: 0\n",
      "Train Loss: 0.4062643446352171\n",
      "Validation Loss: 2.5634671449661255\n",
      "Epoch: 1\n",
      "Train Loss: 0.28549248498419055\n",
      "Validation Loss: 2.7718404283126197\n",
      "Epoch: 2\n",
      "Train Loss: 0.2136614461955817\n",
      "Validation Loss: 2.783754746119181\n",
      "Epoch: 3\n",
      "Train Loss: 0.11153029391299123\n",
      "Validation Loss: 3.050188804666201\n",
      "Epoch: 4\n",
      "Train Loss: 0.09055774865429038\n",
      "Validation Loss: 3.1006506408254304\n",
      "Epoch: 5\n",
      "Train Loss: 0.08374522788369138\n",
      "Validation Loss: 3.4036454061667123\n",
      "Epoch: 6\n",
      "Train Loss: 0.07090433703168579\n",
      "Validation Loss: 3.4428271055221558\n",
      "Epoch: 7\n",
      "Train Loss: 0.06079876601048138\n",
      "Validation Loss: 3.439500709374746\n",
      "Epoch: 8\n",
      "Train Loss: 0.052372267874686615\n",
      "Validation Loss: 3.7107897897561393\n",
      "Epoch: 9\n",
      "Train Loss: 0.05350976728874704\n",
      "Validation Loss: 3.7085403402646384\n",
      "0 83 7 8\n",
      "1 83 7 8\n",
      "2 86 4 5\n",
      "Epoch: 0\n",
      "Train Loss: 0.43009647346557456\n",
      "Validation Loss: 1.5596947843829791\n",
      "Epoch: 1\n",
      "Train Loss: 0.23461079217017966\n",
      "Validation Loss: 1.831321731209755\n",
      "Epoch: 2\n",
      "Train Loss: 0.12043723289636855\n",
      "Validation Loss: 1.7616042718291283\n",
      "Epoch: 3\n",
      "Train Loss: 0.06280430723377999\n",
      "Validation Loss: 2.054170702894529\n",
      "Epoch: 4\n",
      "Train Loss: 0.05523534947411811\n",
      "Validation Loss: 2.220222736398379\n",
      "Epoch: 5\n",
      "Train Loss: 0.04465127078459618\n",
      "Validation Loss: 2.0812980830669403\n",
      "Epoch: 6\n",
      "Train Loss: 0.04027141329455883\n",
      "Validation Loss: 2.4279760171969733\n",
      "Epoch: 7\n",
      "Train Loss: 0.04406068561241982\n",
      "Validation Loss: 2.3421128193537393\n",
      "Epoch: 8\n",
      "Train Loss: 0.0637556717909397\n",
      "Validation Loss: 2.359312822421392\n",
      "Epoch: 9\n",
      "Train Loss: 0.041605045365050754\n",
      "Validation Loss: 2.402901147802671\n",
      "0 79 7 12\n",
      "1 81 5 10\n",
      "2 85 1 6\n",
      "Epoch: 0\n",
      "Train Loss: 0.23729443084448576\n",
      "Validation Loss: 1.7577360272407532\n",
      "Epoch: 1\n",
      "Train Loss: 0.21300807952259979\n",
      "Validation Loss: 1.8475125978390377\n",
      "Epoch: 2\n",
      "Train Loss: 0.07650049558530252\n",
      "Validation Loss: 2.06283862888813\n",
      "Epoch: 3\n",
      "Train Loss: 0.08013880734021465\n",
      "Validation Loss: 1.962350035707156\n",
      "Epoch: 4\n",
      "Train Loss: 0.11562525729338329\n",
      "Validation Loss: 1.837498625119527\n",
      "Epoch: 5\n",
      "Train Loss: 0.061101273323098816\n",
      "Validation Loss: 1.8718637228012085\n",
      "Epoch: 6\n",
      "Train Loss: 0.0315409577281874\n",
      "Validation Loss: 1.8954509099324544\n",
      "Epoch: 7\n",
      "Train Loss: 0.01635692151224551\n",
      "Validation Loss: 2.1266097525755563\n",
      "Epoch: 8\n",
      "Train Loss: 0.013900474606392285\n",
      "Validation Loss: 2.194074869155884\n",
      "Epoch: 9\n",
      "Train Loss: 0.008001460038940422\n",
      "Validation Loss: 2.2646843592325845\n",
      "0 82 6 9\n",
      "1 83 5 8\n",
      "2 86 2 5\n",
      "Epoch: 0\n",
      "Train Loss: 0.34401254627543193\n",
      "Validation Loss: 0.818119635184606\n",
      "Epoch: 1\n",
      "Train Loss: 0.26256808390220004\n",
      "Validation Loss: 0.33378349989652634\n",
      "Epoch: 2\n",
      "Train Loss: 0.10875564472128947\n",
      "Validation Loss: 0.36680106570323306\n",
      "Epoch: 3\n",
      "Train Loss: 0.09529255443097402\n",
      "Validation Loss: 0.8193513552347819\n",
      "Epoch: 4\n",
      "Train Loss: 0.05621232899526755\n",
      "Validation Loss: 0.45944788431127864\n",
      "Epoch: 5\n",
      "Train Loss: 0.058311362750828266\n",
      "Validation Loss: 0.6320553074280421\n",
      "Epoch: 6\n",
      "Train Loss: 0.031526963032471635\n",
      "Validation Loss: 0.543714831272761\n",
      "Epoch: 7\n",
      "Train Loss: 0.03287139923001329\n",
      "Validation Loss: 0.5124793325861295\n",
      "Epoch: 8\n",
      "Train Loss: 0.019810725915400933\n",
      "Validation Loss: 0.5170085405309995\n",
      "Epoch: 9\n",
      "Train Loss: 0.013537830687710084\n",
      "Validation Loss: 0.6849658588568369\n",
      "0 74 8 17\n",
      "1 75 7 16\n",
      "2 78 4 13\n"
     ]
    }
   ],
   "source": [
    "results = train_test_model_across_domains(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model/model.pth')\n",
    "torch.save(glove_embeddings, './model/embeddings.pth')\n",
    "torch.save(model.state_dict(), './model/model_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), './model/encoder_dict.pth')\n",
    "torch.save(decoder.state_dict(), './model/decoder_dict.pth')\n",
    "torch.save(attention.state_dict(), './model/attention_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = torch.load('./model/embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save('../model/model_scripted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087c903105144e0db888efd9ac36a050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aakashsorathiya/req_extraction_seq2seq/commit/d643ee0f7b78d6e57b6fece2d1169fd55ee58984', commit_message='Push model using huggingface_hub.', commit_description='', oid='d643ee0f7b78d6e57b6fece2d1169fd55ee58984', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('aakashsorathiya/req_extraction_seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel.from_pretrained('aakashsorathiya/req_extraction_seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/model.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b44478058948a2ae8af3686c7a3c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.pth:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aakashsorathiya/req_extraction_seq2seq/commit/7efe610105d4722b75143da49a8a5705dfd3b279', commit_message='Upload model.pth with huggingface_hub', commit_description='', oid='7efe610105d4722b75143da49a8a5705dfd3b279', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./model/model.pth\",\n",
    "    path_in_repo=\"model.pth\",\n",
    "    repo_id=\"aakashsorathiya/req_extraction_seq2seq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I']\n"
     ]
    }
   ],
   "source": [
    "inference_sentence = 'The app crashes when I try to share photos with my contacts from another social network'\n",
    "tokens = inference_sentence.split()\n",
    "sentence_idx = [word_to_ix.get(word, word_to_ix['<UNK>']) for word in tokens]\n",
    "sentence_tensor = torch.tensor([sentence_idx], dtype=torch.long)\n",
    "\n",
    "model.eval()\n",
    "emissions = model(sentence_tensor)\n",
    "pred_tags_ix = model.decode(emissions)\n",
    "pred_tags = [ix_to_tag[t] for t in pred_tags_ix[0]]\n",
    "print(pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join('./model/', 'vocabs.json'), 'w') as f:\n",
    "    json.dump({\n",
    "        'input_vocab': word_to_ix,\n",
    "        'target_vocab': tag_to_ix\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
